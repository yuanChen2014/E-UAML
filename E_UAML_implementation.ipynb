{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8adba800",
   "metadata": {},
   "source": [
    "# E-UAML: Multimodal Kinship Verification (Python Skeleton)\n",
    "\n",
    "This notebook provides a **skeleton implementation** of the E-UAML framework in Python using PyTorch. The goal is to demonstrate the architecture described in the manuscript, including:\n",
    "- Modality-specific encoders (face, voice, ear, gait)\n",
    "- L2-normalized embeddings\n",
    "- Adversarial alignment (Gradient Reversal Layer + Modality Discriminator)\n",
    "- Multi-head modality attention\n",
    "- Transformer-based fusion\n",
    "- Composite loss (contrastive + alignment + attention regularization)\n",
    "\n",
    "**Note:** For simplicity, lightweight placeholder models are used. Replace them with real pretrained models (ResNet, VGG, TimeSformer, etc.) for practical training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Embedding sizes for each modality\n",
    "D_FACE = 512\n",
    "D_VOICE = 2048\n",
    "D_EAR = 1024\n",
    "D_GAIT = 512\n",
    "D_FUSED = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf3308",
   "metadata": {},
   "source": [
    "## 1. Modality-Specific Encoders\n",
    "Each modality is encoded into a latent embedding. In the paper, these are:\n",
    "- Face → ResNet-34\n",
    "- Voice → ResNet-50\n",
    "- Ear → VGG-16\n",
    "- Gait → TimeSformer\n",
    "\n",
    "In this notebook, simplified placeholder models are used to demonstrate the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLPEncoder(nn.Module):\n",
    "    \"\"\"Lightweight MLP encoder for vector-like modalities (voice, gait).\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.ReLU(), nn.Linear(512, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SimpleConvEncoder(nn.Module):\n",
    "    \"\"\"Lightweight Conv encoder for image modalities (face, ear).\"\"\"\n",
    "    def __init__(self, in_channels, output_dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(64, output_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.features(x).view(x.size(0), -1)\n",
    "        return self.fc(h)\n",
    "\n",
    "class ModalityEncoders(nn.Module):\n",
    "    \"\"\"Four encoders for four biometric modalities.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.face_encoder = SimpleConvEncoder(3, D_FACE)\n",
    "        self.ear_encoder = SimpleConvEncoder(1, D_EAR)\n",
    "        self.voice_encoder = SimpleMLPEncoder(128, D_VOICE)\n",
    "        self.gait_encoder = SimpleMLPEncoder(128, D_GAIT)\n",
    "    def forward(self, xf, xv, xa, xb):\n",
    "        return (\n",
    "            self.face_encoder(xf),\n",
    "            self.voice_encoder(xv),\n",
    "            self.ear_encoder(xa),\n",
    "            self.gait_encoder(xb)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21758c",
   "metadata": {},
   "source": [
    "## 2. Adversarial Alignment\n",
    "To enforce modality-invariant embeddings, we use:\n",
    "- Gradient Reversal Layer (GRL)\n",
    "- Multi-class modality discriminator\n",
    "\n",
    "This aligns the distributions of embeddings from different modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, λ): ctx.λ = λ; return x.clone()\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return -ctx.λ * g, None\n",
    "\n",
    "def grad_reverse(x, λ=1): return GradientReversalFn.apply(x, λ)\n",
    "\n",
    "class ModalityDiscriminator(nn.Module):\n",
    "    def __init__(self, dim, num_mods=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(dim,256), nn.ReLU(), nn.Linear(256,num_mods))\n",
    "    def forward(self, x): return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27cc11",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Modality Attention\n",
    "This module learns how much each modality should contribute to the final fused representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalityAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model*4), nn.ReLU(), nn.Linear(d_model*4, d_model))\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        h, w = self.attn(x, x, x)\n",
    "        h = self.ln1(x + h)\n",
    "        z = self.ffn(h)\n",
    "        return self.ln2(h + z), w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9deb74f",
   "metadata": {},
   "source": [
    "## 4. Transformer Fusion Layer\n",
    "A lightweight Transformer Encoder integrates the attended embeddings of two individuals (X and Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, d=D_FUSED, heads=8, layers=2):\n",
    "        super().__init__()\n",
    "        L = nn.TransformerEncoderLayer(d, heads, d*4, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(L, layers)\n",
    "    def forward(self, zx, zy):\n",
    "        seq = torch.stack([zx, zy], dim=1)  # (B,2,D)\n",
    "        out = self.encoder(seq)\n",
    "        return out[:,0], out[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260b65f",
   "metadata": {},
   "source": [
    "## 5. Complete E-UAML Model\n",
    "This integrates encoders, attention, fusion, and loss components into a full Siamese-style architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(x, eps=1e-6): return x / (x.norm(2,-1,keepdim=True)+eps)\n",
    "\n",
    "class EUAML(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = ModalityEncoders()\n",
    "        self.attn = ModalityAttention(D_FUSED)\n",
    "        self.fuse = FusionTransformer()\n",
    "        # projection layers\n",
    "        self.pf = nn.Linear(D_FACE, D_FUSED)\n",
    "        self.pv = nn.Linear(D_VOICE, D_FUSED)\n",
    "        self.pa = nn.Linear(D_EAR, D_FUSED)\n",
    "        self.pg = nn.Linear(D_GAIT, D_FUSED)\n",
    "\n",
    "    def encode(self, xf,xv,xa,xb):\n",
    "        ef, ev, ea, eb = self.enc(xf,xv,xa,xb)\n",
    "        return l2_norm(ef), l2_norm(ev), l2_norm(ea), l2_norm(eb)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        efX, evX, eaX, ebX = self.encode(batch['fX'], batch['vX'], batch['aX'], batch['bX'])\n",
    "        efY, evY, eaY, ebY = self.encode(batch['fY'], batch['vY'], batch['aY'], batch['bY'])\n",
    "\n",
    "        zX = torch.stack([\n",
    " self.pf(efX), self.pv(evX), self.pa(eaX), self.pg(ebX)], dim=1)\n",
    "        zY = torch.stack([\n",
    " self.pf(efY), self.pv(evY), self.pa(eaY), self.pg(ebY)], dim=1)\n",
    "\n",
    "        zx, ax = self.attn(zX)\n",
    "        zy, ay = self.attn(zY)\n",
    "\n",
    "        fX, fY = self.fuse(zx.mean(1), zy.mean(1))\n",
    "        return fX, fY, ax, ay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63d687",
   "metadata": {},
   "source": [
    "## 6. Loss Functions\n",
    "Contrastive loss + attention regularization is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88467e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(x,y,labels,margin=1):\n",
    "    d = 1 - F.cosine_similarity(x,y)\n",
    "    pos = labels * d.pow(2)\n",
    "    neg = (1-labels) * F.relu(margin-d).pow(2)\n",
    "    return (pos+neg).mean()\n",
    "\n",
    "def attn_reg(w):\n",
    "    w = w.mean(1).mean(1) # (B,M)\n",
    "    M = w.size(1)\n",
    "    t = torch.full_like(w, 1/M)\n",
    "    return F.mse_loss(w,t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa52627",
   "metadata": {},
   "source": [
    "## 7. Example Training Step (Dummy Data)\n",
    "Random data is used only to demonstrate the forward/backward flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EUAML().to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "B=4\n",
    "batch = {\n",
    "    'fX': torch.randn(B,3,64,64).to(device),\n",
    "    'vX': torch.randn(B,128).to(device),\n",
    "    'aX': torch.randn(B,1,64,64).to(device),\n",
    "    'bX': torch.randn(B,128).to(device),\n",
    "    'fY': torch.randn(B,3,64,64).to(device),\n",
    "    'vY': torch.randn(B,128).to(device),\n",
    "    'aY': torch.randn(B,1,64,64).to(device),\n",
    "    'bY': torch.randn(B,128).to(device),\n",
    "    'labels': torch.randint(0,2,(B,)).float().to(device)\n",
    "}\n",
    "\n",
    "opt.zero_grad()\n",
    "fX,fY,aX,aY = model(batch)\n",
    "L = contrastive_loss(fX,fY,batch['labels']) + 0.01*(attn_reg(aX)+attn_reg(aY))\n",
    "L.backward(); opt.step()\n",
    "print('Loss:', L.item())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
